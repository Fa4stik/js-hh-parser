Первым делом были описаны схемы для api.hh.ru. Схемы были разделены на следующие структуры:
- Вакансии в глобальном поиске
- Информация по конкретной вакансии
- Описание работодателя
- Значения из словарей (индустрия, проф. роль, регион)

Дальше была разработана стратегия получения вакансий:

1) Для начала генерируем ссылки с которых будем получать вакансии
Для одной ссылки с параметрами можно получить не более 2000 тысяч вакансий (100 вакансий на страницу из 20).
В api.hh.ru предусмотрен параметр clusters который возвращает массив возможных параметров, которые помогут сократить количество найденных вакансий
Для примера у нас вакансия с проф. ролью 22. Всего таких найдено 16 тысяч, в clusters возвращаются параметры, которые помогут разбить данную роль на под группы. Если мы взяли параметр к данной группе (в нашем случае группа проф. роль), и их получилось меньше 2.000 этого достаточно, можно собирать информацию тогда.
Простой пример:
parametrs: profession_role=22
found: 16.000
  found > 2.000 
    ? add_cluster_parametr 
    : parse_vacancies() 
Следующий шаг: 
parametrs: 
profession_role=22 & experience=1
... 
profession_role=22 & experience=2 
...
profession_role=22 & experience=3 

И так все параметры обрабатываются рекурсивно пока не будет найдена необходимо количество вакансий в группе, в данном случае <= 2.000

2) С ссылки мы получаем глобальное описание вакансий, откуда достаём ид каждой вакансии
Для общего понимания глобальные вакансии располагаются по пути /vacancies?key=value&...
Конкретная вакансия располагается по пути /vacancies/:id
Разница у них в следующих полях:
Соискатель с инвалидностью (accept_handicapped)
Соискатель старше 14 лет (accept_kids)
Разрешить сообщения (allow_messages)
Прошла ли вакансия модерацию (approved)
Биллинговый тип из справочника (billing_type)
Внутренний код вакансии (code)
Описание вакансии (description)
Список требуемых категорий водительских прав (driver_license_types)
Дата и время создания вакансии (initial_created_at)
Список ключевых навыков, не более 30 (key_skills)
Языки вакансии (languages)
Ссылка для получения списка откликов/приглашений (negotiations_url)
Подходящие резюме на вакансию (suitable_resumes_url)
Тест, который будет добавлен в вакансию (test)
Дата создания (created_at)
Информация о метро (metro_stations)
Расстояние в метрах между центром сортировки (заданной параметрами sort_point_lat, sort_point_lng) и указанным в вакансии адресом (sort_point_distance)
Ссылка на вакансию (url)
Количество откликов на вакансию с момента публикации (counters)
Отрывок из требований по вакансии, если они найдены в тексте описания (snippet)

3) Делаем запрос на получение каждой вакансии
При этом важно отметить, что 2.000 вакансий в среднем будет обрабатываться 16 минут. 
Проблема заключается в капче которую вы возможно получите, если будете делать запросы слишком часто. Чтобы избежать капчу необходимо после получения информации с каждой страницы ждать 1 минуту. Конечно, для тысячи вакансий это не большое время, но если у вас больше тут начинаются проблемы. 
Самым простым решением является использовать прокси.
Также такой большой объём данных держать в оперативной памяти тоже не лучшая идея, потому что у вас попросту может не хватить места и лучшим решением является сохранять каждую страницу на жёсткий диск и потом полученные данные объединить

4) Получаем список уникальных ид упомянутых работодателей
Некоторые работодатели могут публиковать разные вакансии, поэтому важно исключить тех, кто уже был упомянут в вакансиях

5) Получаем информацию по каждому работодателю через api
Стратегия такая же как и в п.3

6) Получаем html-страницу работодателя на hh.ru и парсим её
Проблема в том, что некоторой информации попросту нет в api: рейтинг (от 0 до 5), преимущества сформированные на отзывах сотрудниках, количество отзывов, рейтинги "dream job" (от 0 до 100)

7) Во всех колонка где получен массив, разбиваем на 2 таблицы: перечисление уникальных полей и все перечисленные варианты значений (при этом [{ id: 1}, { id: 2 }] и [{ id: 2, id: 1 }] - одинаковые значения)
8) Заменяем массив на выше созданное перечисление (его идентификатором)
Пункты 7, 8 позволяют нормализировать полученные данные

