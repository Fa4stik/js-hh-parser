import json
import os
from pathlib import Path
from typing import List, Dict

from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
import uvicorn

# –ö–æ–Ω—Å—Ç–∞–Ω—Ç–∞ –¥–ª—è –∫–µ—à–∞ –º–æ–¥–µ–ª–∏
CACHE_DIR = "/mnt/kernai_storage02/s.v.sharifulin/model_cache"

# Pydantic –º–æ–¥–µ–ª–∏ –¥–ª—è request/response
class VacancyRequest(BaseModel):
    body: str


class SkillsResponse(BaseModel):
    soft: List[str]
    hard: List[str]


class QwenSkillExtractor:
    def __init__(self):
        self.model = None
        self.tokenizer = None
        self.soft_skills = []
        self.hard_skills = []
        self.prompt_template = ""
        self._load_skills_and_prompt()
        
    def _load_skills_and_prompt(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –Ω–∞–≤—ã–∫–∏ –∏ –ø—Ä–æ–º—Ç –∏–∑ —Ñ–∞–π–ª–æ–≤"""
        base_path = Path(__file__).parent.parent
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å–æ—Ñ—Ç-—Å–∫–∏–ª–ª—ã
        soft_path = base_path / "disco" / "skils" / "soft.txt"
        with open(soft_path, 'r', encoding='utf-8') as f:
            self.soft_skills = [line.strip() for line in f.readlines() if line.strip()]
            
        # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ö–∞—Ä–¥-—Å–∫–∏–ª–ª—ã
        hard_path = base_path / "disco" / "skils" / "hard.txt"
        with open(hard_path, 'r', encoding='utf-8') as f:
            self.hard_skills = [line.strip() for line in f.readlines() if line.strip()]
            
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –ø—Ä–æ–º—Ç
        prompt_path = base_path / "ai" / "promt.txt"
        with open(prompt_path, 'r', encoding='utf-8') as f:
            self.prompt_template = f.read().strip()
    
    def _get_device_info(self) -> str:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ–± —É—Å—Ç—Ä–æ–π—Å—Ç–≤–µ –º–æ–¥–µ–ª–∏"""
        if self.model is None:
            return "–ú–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞"
        
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å CUDA
            cuda_available = torch.cuda.is_available()
            
            if hasattr(self.model, 'hf_device_map') and self.model.hf_device_map:
                # –ï—Å–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è device_map, –ø–æ–∫–∞–∑—ã–≤–∞–µ–º —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ
                device_map = self.model.hf_device_map
                devices = list(set(str(dev) for dev in device_map.values() if dev != 'disk'))
                
                if any('cuda' in dev for dev in devices):
                    gpu_devices = [dev for dev in devices if 'cuda' in dev]
                    if len(gpu_devices) == 1:
                        gpu_name = torch.cuda.get_device_name(0) if cuda_available else "GPU"
                        return f"GPU ({gpu_name})"
                    else:
                        return f"–ú—É–ª—å—Ç–∏-GPU ({', '.join(gpu_devices)})"
                else:
                    return "CPU"
            else:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –ø–µ—Ä–≤–æ–≥–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞ –º–æ–¥–µ–ª–∏
                first_param_device = next(self.model.parameters()).device
                if first_param_device.type == 'cuda':
                    gpu_name = torch.cuda.get_device_name(first_param_device.index) if cuda_available else "GPU"
                    return f"GPU ({gpu_name})"
                else:
                    return "CPU"
                    
        except Exception as e:
            return f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ (–æ—à–∏–±–∫–∞: {e})"
    
    def _load_model(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –º–æ–¥–µ–ª—å Qwen3-8B"""
        if self.model is None:
            print("–ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å Qwen3-8B...")
            model_name = "Qwen/Qwen3-8B"
            
            # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –∫–µ—à–∞, –µ—Å–ª–∏ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
            cache_dir = Path(CACHE_DIR)
            cache_dir.mkdir(parents=True, exist_ok=True)
            print(f"–ö–µ—à –º–æ–¥–µ–ª–∏ –±—É–¥–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤: {cache_dir}")
            
            try:
                # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä
                self.tokenizer = AutoTokenizer.from_pretrained(
                    model_name,
                    cache_dir=CACHE_DIR
                )
                
                # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å
                self.model = AutoModelForCausalLM.from_pretrained(
                    model_name,
                    torch_dtype="auto",
                    device_map="auto",
                    cache_dir=CACHE_DIR
                )
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –º–æ–¥–µ–ª–∏
                device_info = self._get_device_info()
                print(f"–ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ")
                print(f"üñ•Ô∏è  –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device_info}")
                
            except Exception as e:
                error_msg = str(e)
                if "Qwen2Tokenizer" in error_msg:
                    print("‚ùå –û—à–∏–±–∫–∞: –£—Å—Ç–∞—Ä–µ–≤—à–∞—è –≤–µ—Ä—Å–∏—è transformers!")
                    print("üìã –†–µ—à–µ–Ω–∏–µ: –û–±–Ω–æ–≤–∏—Ç–µ transformers –¥–æ –≤–µ—Ä—Å–∏–∏ >= 4.51.0")
                    print("üîß –ö–æ–º–∞–Ω–¥–∞: pip install transformers>=4.51.0 --upgrade")
                    print("üöÄ –ò–ª–∏ –∑–∞–ø—É—Å—Ç–∏—Ç–µ: python update_requirements.py")
                elif "Connection" in error_msg or "timeout" in error_msg.lower():
                    print("‚ùå –û—à–∏–±–∫–∞: –ü—Ä–æ–±–ª–µ–º—ã —Å –∏–Ω—Ç–µ—Ä–Ω–µ—Ç —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ–º")
                    print("üìã –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∏ –ø–æ–≤—Ç–æ—Ä–∏—Ç–µ –ø–æ–ø—ã—Ç–∫—É")
                else:
                    print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –º–æ–¥–µ–ª–∏: {e}")
                raise
    
    def _format_skills_list(self, skills: List[str]) -> str:
        """–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç —Å–ø–∏—Å–æ–∫ –Ω–∞–≤—ã–∫–æ–≤ –¥–ª—è –ø—Ä–æ–º—Ç–∞"""
        return "\n".join(f"- {skill}" for skill in skills)
    
    def _prepare_prompt(self, description: str) -> str:
        """–ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ—Ç –ø—Ä–æ–º—Ç —Å –∑–∞–º–µ–Ω–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö"""
        soft_formatted = self._format_skills_list(self.soft_skills)
        hard_formatted = self._format_skills_list(self.hard_skills)
        
        prompt = self.prompt_template.replace("${description}", description)
        prompt = prompt.replace("${soft}", soft_formatted)
        prompt = prompt.replace("${hard}", hard_formatted)
        
        return prompt
    
    def _parse_model_response(self, response: str) -> Dict[str, List[str]]:
        """–ü–∞—Ä—Å–∏—Ç –æ—Ç–≤–µ—Ç –º–æ–¥–µ–ª–∏ –∏ –∏–∑–≤–ª–µ–∫–∞–µ—Ç JSON"""
        try:
            # –ò—â–µ–º JSON –≤ –æ—Ç–≤–µ—Ç–µ
            start_idx = response.find('{')
            end_idx = response.rfind('}')
            
            if start_idx == -1 or end_idx == -1:
                raise ValueError("JSON –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ –æ—Ç–≤–µ—Ç–µ –º–æ–¥–µ–ª–∏")
            
            json_str = response[start_idx:end_idx + 1]
            result = json.loads(json_str)
            
            # –í–∞–ª–∏–¥–∏—Ä—É–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É
            if not isinstance(result.get('soft'), list) or not isinstance(result.get('hard'), list):
                raise ValueError("–ù–µ–≤–µ—Ä–Ω–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ JSON –æ—Ç–≤–µ—Ç–∞")
            
            # –§–∏–ª—å—Ç—Ä—É–µ–º –Ω–∞–≤—ã–∫–∏ - –æ—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ
            filtered_result = self._validate_and_filter_skills(result)
            
            return filtered_result
            
        except (json.JSONDecodeError, ValueError) as e:
            print(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –æ—Ç–≤–µ—Ç–∞ –º–æ–¥–µ–ª–∏: {e}")
            print(f"–û—Ç–≤–µ—Ç –º–æ–¥–µ–ª–∏: {response}")
            
            # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—É—Å—Ç–æ–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏
            return {"soft": [], "hard": []}
    
    def _validate_and_filter_skills(self, result: Dict[str, List[str]]) -> Dict[str, List[str]]:
        """–í–∞–ª–∏–¥–∏—Ä—É–µ—Ç –∏ —Ñ–∏–ª—å—Ç—Ä—É–µ—Ç –Ω–∞–≤—ã–∫–∏, –æ—Å—Ç–∞–≤–ª—è—è —Ç–æ–ª—å–∫–æ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –∏ —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ"""
        
        def normalize_skill(skill: str) -> str:
            """–ù–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç –Ω–∞–≤—ã–∫ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è"""
            return skill.strip().lower()
        
        def find_exact_skill(skill_to_find: str, skills_list: List[str]) -> str:
            """–ù–∞—Ö–æ–¥–∏—Ç —Ç–æ—á–Ω–æ–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –Ω–∞–≤—ã–∫–∞ –≤ —Å–ø–∏—Å–∫–µ"""
            normalized_to_find = normalize_skill(skill_to_find)
            for original_skill in skills_list:
                if normalize_skill(original_skill) == normalized_to_find:
                    return original_skill
            return None
        
        def remove_duplicates(skills_list: List[str]) -> List[str]:
            """–£–¥–∞–ª—è–µ—Ç –¥—É–±–ª–∏–∫–∞—Ç—ã, —Å–æ—Ö—Ä–∞–Ω—è—è –ø–æ—Ä—è–¥–æ–∫ –∏ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π —Ä–µ–≥–∏—Å—Ç—Ä"""
            seen = set()
            unique_skills = []
            for skill in skills_list:
                normalized = normalize_skill(skill)
                if normalized not in seen:
                    seen.add(normalized)
                    unique_skills.append(skill)
            return unique_skills
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º —Å–æ—Ñ—Ç-—Å–∫–∏–ª–ª—ã
        filtered_soft = []
        for skill in result.get('soft', []):
            exact_skill = find_exact_skill(skill, self.soft_skills)
            if exact_skill:
                filtered_soft.append(exact_skill)
            else:
                print(f"‚ùå –ù–∞–≤—ã–∫ '{skill}' –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ —Å–ø–∏—Å–∫–µ —Å–æ—Ñ—Ç-—Å–∫–∏–ª–ª–æ–≤")
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º —Ö–∞—Ä–¥-—Å–∫–∏–ª–ª—ã
        filtered_hard = []
        for skill in result.get('hard', []):
            exact_skill = find_exact_skill(skill, self.hard_skills)
            if exact_skill:
                filtered_hard.append(exact_skill)
            else:
                print(f"‚ùå –ù–∞–≤—ã–∫ '{skill}' –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ —Å–ø–∏—Å–∫–µ —Ö–∞—Ä–¥-—Å–∫–∏–ª–ª–æ–≤")
        
        # –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
        soft_before_dedup = len(filtered_soft)
        hard_before_dedup = len(filtered_hard)
        
        unique_soft = remove_duplicates(filtered_soft)
        unique_hard = remove_duplicates(filtered_hard)
        
        # –õ–æ–≥–∏—Ä—É–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –¥—É–±–ª–∏–∫–∞—Ç–∞—Ö
        soft_duplicates = soft_before_dedup - len(unique_soft)
        hard_duplicates = hard_before_dedup - len(unique_hard)
        
        if soft_duplicates > 0:
            print(f"üîÑ –£–¥–∞–ª–µ–Ω–æ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ —Å–æ—Ñ—Ç-—Å–∫–∏–ª–ª–æ–≤: {soft_duplicates}")
        if hard_duplicates > 0:
            print(f"üîÑ –£–¥–∞–ª–µ–Ω–æ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ —Ö–∞—Ä–¥-—Å–∫–∏–ª–ª–æ–≤: {hard_duplicates}")
        
        print(f"‚úÖ –í–∞–ª–∏–¥–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞. –°–æ—Ñ—Ç: {len(unique_soft)}/{len(result.get('soft', []))}, –•–∞—Ä–¥: {len(unique_hard)}/{len(result.get('hard', []))}")
        
        return {
            "soft": unique_soft,
            "hard": unique_hard
        }
    
    def extract_skills(self, description: str) -> Dict[str, List[str]]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –Ω–∞–≤—ã–∫–∏ –∏–∑ –æ–ø–∏—Å–∞–Ω–∏—è –≤–∞–∫–∞–Ω—Å–∏–∏"""
        self._load_model()
        
        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –ø—Ä–æ–º—Ç
        prompt = self._prepare_prompt(description)
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º —Å–æ–æ–±—â–µ–Ω–∏—è –¥–ª—è —á–∞—Ç–∞
        messages = [
            {"role": "user", "content": prompt}
        ]
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º —à–∞–±–ª–æ–Ω —á–∞—Ç–∞
        text = self.tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True,
            enable_thinking=False  # –û—Ç–∫–ª—é—á–∞–µ–º thinking mode –¥–ª—è –ø—Ä–æ—Å—Ç–æ—Ç—ã
        )
        
        # –¢–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ–º
        model_inputs = self.tokenizer([text], return_tensors="pt").to(self.model.device)
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ –¥–ª—è non-thinking mode
        with torch.no_grad():
            generated_ids = self.model.generate(
                **model_inputs,
                max_new_tokens=1000,
                temperature=0.7,
                top_p=0.8,
                top_k=20,
                do_sample=True,
                pad_token_id=self.tokenizer.eos_token_id
            )
        
        # –î–µ–∫–æ–¥–∏—Ä—É–µ–º —Ç–æ–ª—å–∫–æ –Ω–æ–≤—É—é —á–∞—Å—Ç—å
        output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()
        response = self.tokenizer.decode(output_ids, skip_special_tokens=True).strip()
        
        # –ü–∞—Ä—Å–∏–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        return self._parse_model_response(response)


# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —ç–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä –Ω–∞–≤—ã–∫–æ–≤
skill_extractor = QwenSkillExtractor()

# –°–æ–∑–¥–∞–µ–º FastAPI –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ
app = FastAPI(
    title="Vacancy Skills Extractor API",
    description="API –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –Ω–∞–≤—ã–∫–æ–≤ –∏–∑ –æ–ø–∏—Å–∞–Ω–∏–π –≤–∞–∫–∞–Ω—Å–∏–π —Å –ø–æ–º–æ—â—å—é Qwen3-8B",
    version="1.0.0"
)


@app.get("/")
async def root():
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–±–æ—Ç–æ—Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ API"""
    return {"message": "Vacancy Skills Extractor API —Ä–∞–±–æ—Ç–∞–µ—Ç"}


@app.post("/api/vacancy", response_model=SkillsResponse)
async def extract_vacancy_skills(request: VacancyRequest):
    """
    –ò–∑–≤–ª–µ–∫–∞–µ—Ç –Ω–∞–≤—ã–∫–∏ –∏–∑ –æ–ø–∏—Å–∞–Ω–∏—è –≤–∞–∫–∞–Ω—Å–∏–∏
    
    Args:
        request: –û–±—ä–µ–∫—Ç —Å –æ–ø–∏—Å–∞–Ω–∏–µ–º –≤–∞–∫–∞–Ω—Å–∏–∏
    
    Returns:
        SkillsResponse: –û–±—ä–µ–∫—Ç —Å —Å–ø–∏—Å–∫–∞–º–∏ —Å–æ—Ñ—Ç –∏ —Ö–∞—Ä–¥ –Ω–∞–≤—ã–∫–æ–≤
    """
    try:
        if not request.body.strip():
            raise HTTPException(status_code=400, detail="–û–ø–∏—Å–∞–Ω–∏–µ –≤–∞–∫–∞–Ω—Å–∏–∏ –Ω–µ –º–æ–∂–µ—Ç –±—ã—Ç—å –ø—É—Å—Ç—ã–º")
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –Ω–∞–≤—ã–∫–∏
        skills = skill_extractor.extract_skills(request.body)
        
        return SkillsResponse(
            soft=skills.get("soft", []),
            hard=skills.get("hard", [])
        )
        
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –≤–∞–∫–∞–Ω—Å–∏–∏: {e}")
        raise HTTPException(status_code=500, detail=f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏: {str(e)}")


@app.get("/health")
async def health_check():
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ—Å—Ç–æ—è–Ω–∏—è –º–æ–¥–µ–ª–∏"""
    try:
        model_loaded = skill_extractor.model is not None
        return {
            "status": "healthy",
            "model_loaded": model_loaded,
            "soft_skills_count": len(skill_extractor.soft_skills),
            "hard_skills_count": len(skill_extractor.hard_skills)
        }
    except Exception as e:
        return {"status": "unhealthy", "error": str(e)}


if __name__ == "__main__":
    # –ó–∞–ø—É—Å–∫–∞–µ–º —Å–µ—Ä–≤–µ—Ä
    uvicorn.run(
        "qwen:app",
        host="0.0.0.0", 
        port=8000,
        reload=True
    )
