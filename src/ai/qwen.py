import json
import os
from pathlib import Path
from typing import List, Dict

from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
import uvicorn

# –ö–æ–Ω—Å—Ç–∞–Ω—Ç–∞ –¥–ª—è –∫–µ—à–∞ –º–æ–¥–µ–ª–∏
CACHE_DIR = "/mnt/kernai_storage02/s.v.sharifulin/model_cache"

# Pydantic –º–æ–¥–µ–ª–∏ –¥–ª—è request/response
class VacancyRequest(BaseModel):
    body: str


class SkillsResponse(BaseModel):
    soft: List[str]
    hard: List[str]


class QwenSkillExtractor:
    def __init__(self):
        self.model = None
        self.tokenizer = None
        self.soft_skills = []
        self.hard_skills = []
        self.prompt_template = ""
        self._load_skills_and_prompt()
        
    def _load_skills_and_prompt(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –Ω–∞–≤—ã–∫–∏ –∏ –ø—Ä–æ–º—Ç –∏–∑ —Ñ–∞–π–ª–æ–≤"""
        base_path = Path(__file__).parent.parent
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å–æ—Ñ—Ç-—Å–∫–∏–ª–ª—ã
        soft_path = base_path / "disco" / "skils" / "soft.txt"
        with open(soft_path, 'r', encoding='utf-8') as f:
            self.soft_skills = [line.strip() for line in f.readlines() if line.strip()]
            
        # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ö–∞—Ä–¥-—Å–∫–∏–ª–ª—ã
        hard_path = base_path / "disco" / "skils" / "hard.txt"
        with open(hard_path, 'r', encoding='utf-8') as f:
            self.hard_skills = [line.strip() for line in f.readlines() if line.strip()]
            
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –ø—Ä–æ–º—Ç
        prompt_path = base_path / "ai" / "promt.txt"
        with open(prompt_path, 'r', encoding='utf-8') as f:
            self.prompt_template = f.read().strip()
    
    def _load_model(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –º–æ–¥–µ–ª—å Qwen3-8B"""
        if self.model is None:
            print("–ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å Qwen3-8B...")
            model_name = "Qwen/Qwen3-8B"
            
            # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –∫–µ—à–∞, –µ—Å–ª–∏ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
            cache_dir = Path(CACHE_DIR)
            cache_dir.mkdir(parents=True, exist_ok=True)
            print(f"–ö–µ—à –º–æ–¥–µ–ª–∏ –±—É–¥–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤: {cache_dir}")
            
            try:
                # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä
                self.tokenizer = AutoTokenizer.from_pretrained(
                    model_name,
                    cache_dir=CACHE_DIR
                )
                
                # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å
                self.model = AutoModelForCausalLM.from_pretrained(
                    model_name,
                    torch_dtype="auto",
                    device_map="auto",
                    cache_dir=CACHE_DIR
                )
                
                print(f"–ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ")
                
            except Exception as e:
                error_msg = str(e)
                if "Qwen2Tokenizer" in error_msg:
                    print("‚ùå –û—à–∏–±–∫–∞: –£—Å—Ç–∞—Ä–µ–≤—à–∞—è –≤–µ—Ä—Å–∏—è transformers!")
                    print("üìã –†–µ—à–µ–Ω–∏–µ: –û–±–Ω–æ–≤–∏—Ç–µ transformers –¥–æ –≤–µ—Ä—Å–∏–∏ >= 4.51.0")
                    print("üîß –ö–æ–º–∞–Ω–¥–∞: pip install transformers>=4.51.0 --upgrade")
                    print("üöÄ –ò–ª–∏ –∑–∞–ø—É—Å—Ç–∏—Ç–µ: python update_requirements.py")
                elif "Connection" in error_msg or "timeout" in error_msg.lower():
                    print("‚ùå –û—à–∏–±–∫–∞: –ü—Ä–æ–±–ª–µ–º—ã —Å –∏–Ω—Ç–µ—Ä–Ω–µ—Ç —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ–º")
                    print("üìã –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∏ –ø–æ–≤—Ç–æ—Ä–∏—Ç–µ –ø–æ–ø—ã—Ç–∫—É")
                else:
                    print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –º–æ–¥–µ–ª–∏: {e}")
                raise
    
    def _format_skills_list(self, skills: List[str]) -> str:
        """–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç —Å–ø–∏—Å–æ–∫ –Ω–∞–≤—ã–∫–æ–≤ –¥–ª—è –ø—Ä–æ–º—Ç–∞"""
        return "\n".join(f"- {skill}" for skill in skills)
    
    def _prepare_prompt(self, description: str) -> str:
        """–ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ—Ç –ø—Ä–æ–º—Ç —Å –∑–∞–º–µ–Ω–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö"""
        soft_formatted = self._format_skills_list(self.soft_skills)
        hard_formatted = self._format_skills_list(self.hard_skills)
        
        prompt = self.prompt_template.replace("${description}", description)
        prompt = prompt.replace("${soft}", soft_formatted)
        prompt = prompt.replace("${hard}", hard_formatted)
        
        return prompt
    
    def _parse_model_response(self, response: str) -> Dict[str, List[str]]:
        """–ü–∞—Ä—Å–∏—Ç –æ—Ç–≤–µ—Ç –º–æ–¥–µ–ª–∏ –∏ –∏–∑–≤–ª–µ–∫–∞–µ—Ç JSON"""
        try:
            # –ò—â–µ–º JSON –≤ –æ—Ç–≤–µ—Ç–µ
            start_idx = response.find('{')
            end_idx = response.rfind('}')
            
            if start_idx == -1 or end_idx == -1:
                raise ValueError("JSON –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ –æ—Ç–≤–µ—Ç–µ –º–æ–¥–µ–ª–∏")
            
            json_str = response[start_idx:end_idx + 1]
            result = json.loads(json_str)
            
            # –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º –æ–±–∞ –≤–∞—Ä–∏–∞–Ω—Ç–∞ –∫–ª—é—á–µ–π: "soft"/"hard" –∏ "soft_skills"/"hard_skills"
            soft_skills = []
            hard_skills = []
            
            if 'soft' in result and isinstance(result['soft'], list):
                soft_skills = result['soft']
            elif 'soft_skills' in result and isinstance(result['soft_skills'], list):
                soft_skills = result['soft_skills']
            
            if 'hard' in result and isinstance(result['hard'], list):
                hard_skills = result['hard']
            elif 'hard_skills' in result and isinstance(result['hard_skills'], list):
                hard_skills = result['hard_skills']
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —Ö–æ—Ç—è –±—ã –æ–¥–∏–Ω —Ç–∏–ø –Ω–∞–≤—ã–∫–æ–≤ –Ω–∞–π–¥–µ–Ω
            if not soft_skills and not hard_skills:
                raise ValueError("–ù–µ –Ω–∞–π–¥–µ–Ω—ã –Ω–∞–≤—ã–∫–∏ –≤ –æ—Ç–≤–µ—Ç–µ –º–æ–¥–µ–ª–∏")
            
            return {"soft": soft_skills, "hard": hard_skills}
            
        except (json.JSONDecodeError, ValueError) as e:
            print(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –æ—Ç–≤–µ—Ç–∞ –º–æ–¥–µ–ª–∏: {e}")
            print(f"–û—Ç–≤–µ—Ç –º–æ–¥–µ–ª–∏: {response}")
            
            # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—É—Å—Ç–æ–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏
            return {"soft": [], "hard": []}
    
    def extract_skills(self, description: str) -> Dict[str, List[str]]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –Ω–∞–≤—ã–∫–∏ –∏–∑ –æ–ø–∏—Å–∞–Ω–∏—è –≤–∞–∫–∞–Ω—Å–∏–∏"""
        self._load_model()
        
        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –ø—Ä–æ–º—Ç
        prompt = self._prepare_prompt(description)
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º —Å–æ–æ–±—â–µ–Ω–∏—è –¥–ª—è —á–∞—Ç–∞
        messages = [
            {"role": "user", "content": prompt}
        ]
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º —à–∞–±–ª–æ–Ω —á–∞—Ç–∞
        text = self.tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True,
            enable_thinking=False  # –û—Ç–∫–ª—é—á–∞–µ–º thinking mode –¥–ª—è –ø—Ä–æ—Å—Ç–æ—Ç—ã
        )
        
        # –¢–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ–º
        model_inputs = self.tokenizer([text], return_tensors="pt").to(self.model.device)
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ –¥–ª—è non-thinking mode
        with torch.no_grad():
            generated_ids = self.model.generate(
                **model_inputs,
                max_new_tokens=1000,
                temperature=0.7,
                top_p=0.8,
                top_k=20,
                do_sample=True,
                pad_token_id=self.tokenizer.eos_token_id
            )
        
        # –î–µ–∫–æ–¥–∏—Ä—É–µ–º —Ç–æ–ª—å–∫–æ –Ω–æ–≤—É—é —á–∞—Å—Ç—å
        output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()
        response = self.tokenizer.decode(output_ids, skip_special_tokens=True).strip()
        
        # –ü–∞—Ä—Å–∏–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        return self._parse_model_response(response)


# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —ç–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä –Ω–∞–≤—ã–∫–æ–≤
skill_extractor = QwenSkillExtractor()

# –°–æ–∑–¥–∞–µ–º FastAPI –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ
app = FastAPI(
    title="Vacancy Skills Extractor API",
    description="API –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –Ω–∞–≤—ã–∫–æ–≤ –∏–∑ –æ–ø–∏—Å–∞–Ω–∏–π –≤–∞–∫–∞–Ω—Å–∏–π —Å –ø–æ–º–æ—â—å—é Qwen3-8B",
    version="1.0.0"
)


@app.get("/")
async def root():
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–±–æ—Ç–æ—Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ API"""
    return {"message": "Vacancy Skills Extractor API —Ä–∞–±–æ—Ç–∞–µ—Ç"}


@app.post("/api/vacancy", response_model=SkillsResponse)
async def extract_vacancy_skills(request: VacancyRequest):
    """
    –ò–∑–≤–ª–µ–∫–∞–µ—Ç –Ω–∞–≤—ã–∫–∏ –∏–∑ –æ–ø–∏—Å–∞–Ω–∏—è –≤–∞–∫–∞–Ω—Å–∏–∏
    
    Args:
        request: –û–±—ä–µ–∫—Ç —Å –æ–ø–∏—Å–∞–Ω–∏–µ–º –≤–∞–∫–∞–Ω—Å–∏–∏
    
    Returns:
        SkillsResponse: –û–±—ä–µ–∫—Ç —Å —Å–ø–∏—Å–∫–∞–º–∏ —Å–æ—Ñ—Ç –∏ —Ö–∞—Ä–¥ –Ω–∞–≤—ã–∫–æ–≤
    """
    try:
        if not request.body.strip():
            raise HTTPException(status_code=400, detail="–û–ø–∏—Å–∞–Ω–∏–µ –≤–∞–∫–∞–Ω—Å–∏–∏ –Ω–µ –º–æ–∂–µ—Ç –±—ã—Ç—å –ø—É—Å—Ç—ã–º")
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –Ω–∞–≤—ã–∫–∏
        skills = skill_extractor.extract_skills(request.body)
        
        return SkillsResponse(
            soft=skills.get("soft", []),
            hard=skills.get("hard", [])
        )
        
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –≤–∞–∫–∞–Ω—Å–∏–∏: {e}")
        raise HTTPException(status_code=500, detail=f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏: {str(e)}")


@app.get("/health")
async def health_check():
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ—Å—Ç–æ—è–Ω–∏—è –º–æ–¥–µ–ª–∏"""
    try:
        model_loaded = skill_extractor.model is not None
        return {
            "status": "healthy",
            "model_loaded": model_loaded,
            "soft_skills_count": len(skill_extractor.soft_skills),
            "hard_skills_count": len(skill_extractor.hard_skills)
        }
    except Exception as e:
        return {"status": "unhealthy", "error": str(e)}


if __name__ == "__main__":
    # –ó–∞–ø—É—Å–∫–∞–µ–º —Å–µ—Ä–≤–µ—Ä
    uvicorn.run(
        "qwen:app",
        host="0.0.0.0", 
        port=8000,
        reload=True
    )
